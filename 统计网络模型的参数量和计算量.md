
>在神经网络中，参数量和计算量是衡量神经网络模型的两个重要指标；参数量决定网络的大小，其主要影响的是模型的内存或显存大小；计算量则决定网络运行的快慢；

一、计算说明：

参数量（#paras）即为网络模型中含有多少个参数，与输入的数据无关，主要与模型的结构有关系；其主要影响模型运算是所需要的内存或显存

计算量(#FLOPs)通常使用FLOPs(Floating point operations，浮点运算数量)来表示计算量，其主要来衡量算法/模型的复杂度。论文中一般用GFLOPs来表示，1GFLOPs=10^9 FLOPs；

注：FLOPS和FLOPs是代表不同的意义的，FLOPS(floating point operations per second),是指每秒浮点运算次数，可以理解成计算速度，是衡量一个硬件性能的指标，如高通865的npu号称支持10TFLOPS的计算性能；

对于一个卷积层，假设其大小为：h * w * c * n (其中c为：input channel, n为：output channel , h w为kernel的尺寸),输出的feature map尺寸为H * W；
```
#paras =  n * (h * w *c + 1) 

#FLOPs = H * W  * n * (h * w  * c       +1)
```
更多涉及计算量和参数量的详细介绍，可以参考：https://www.zhihu.com/question/65305385/answer/641705098

二、具体计算
2.1、TensorFlow中统计参数量和计算量：

针对于tensorflow1.x，tf官方提供了计算参数量和计算量的接口，使用方法如下：

1、定义网络
```
model=SEG_MODEL(cfg) # 网络对应的class

with tf.Graph().as_default() as graph:

      img_placeholder = tf.placeholder(tf.float32,shape=[1,INPUT_HEIGHT,INPUT_WIDTH,3],name='img_placeholder')

      pred=model.get_logits(img_placeholder) #获得网络的模型结构

    print('stats before freezing')

      stats_graph(graph)
```
2、统计计算量
```
def stats_graph(graph):

    flops = tf.profiler.profile(graph,options=tf.profiler.ProfileOptionBuilder.float_operation())

    params = tf.profiler.profile(graph,options=tf.profiler.ProfileOptionBuilder.trainable_variables_parameter())

    flops_num = flops.total_float_ops

    total_param = params.total_parameters

    print('GFLOPs:{};Trainableparams(M):{}'.format(flops_num/1e9,total_param/1e6))

    return flops_num,total_param
```
这样既可获得网络模型的参数量和计算量：

3、计算量结果统计：

如下为BiSeNetV2中的计算量统计结果：
```
Profile:

nodename|#float_ops

_TFProfRoot(--/16.03bflops)

BiSeNetV2/DetailBranch/Conv_1/Conv2D(4.83b/4.83bflops)

BiSeNetV2/DetailBranch/Conv_3/Conv2D(1.21b/1.21bflops)

BiSeNetV2/DetailBranch/Conv_4/Conv2D(1.21b/1.21bflops)

BiSeNetV2/DetailBranch/Conv_2/Conv2D(1.21b/1.21bflops)

BiSeNetV2/DetailBranch/Conv_6/Conv2D(1.21b/1.21bflops)

BiSeNetV2/DetailBranch/Conv_7/Conv2D(1.21b/1.21bflops)

BiSeNetV2/AggregationLayer/AggConv5/Conv2D(1.21b/1.21bflops)

BiSeNetV2/DetailBranch/Conv_5/Conv2D(603.98m/603.98mflops)

BiSeNetV2/SegHeadEnd/Conv/Conv2D(603.98m/603.98mflops)

BiSeNetV2/AggregationLayer/AggConv2/Conv2D(301.99m/301.99mflops)

BiSeNetV2/SegHeadS4/Conv/Conv2D(301.99m/301.99mflops)

BiSeNetV2/DetailBranch/Conv/Conv2D(226.49m/226.49mflops)

BiSeNetV2/SemanticBranch/StemBlock/Conv_3/Conv2D(150.99m/150.99mflops)

BiSeNetV2/SegHeadS8/Conv/Conv2D(150.99m/150.99mflops)

BiSeNetV2/AggregationLayer/AggConv1/Conv2D(134.22m/134.22mflops)

BiSeNetV2/AggregationLayer/AggConv4/Conv2D(75.50m/75.50mflops)

BiSeNetV2/SemanticBranch/GatherExpansionLayer_6/Conv/Conv2D(75.50m/75.50mflops)

BiSeNetV2/SemanticBranch/GatherExpansionLayer_1/Conv/Conv2D(75.50m/75.50mflops)
```

>注：
>16.03bflops即为 16.03      billion flops =16.03 * 10^9FLOPs =       16.03GFLOPs
>603.98mflops即为603.98 million flops = 0.60398GFLOPs

上述结果中：_TFProfRoot(--/16.03bflops) 即为整个网络从输入到输出的计算量为16.03GFLOPs；BiSeNetV2/DetailBranch/Conv_1/Conv2D(4.83b/4.83bflops)：前面表示网络中某一层的node的name,4.83bflops即为当前层的计算量；这里计算量会按照每个node从大到小排序；

4、参数量结果统计：

如下为BiSeNetV2中统计的参数量：
```
nodename|#parameters

_TFProfRoot(--/2.39mparams)

AggregationLayer(--/480.00kparams)

AggregationLayer/AggConv1(--/16.38kparams)

AggregationLayer/AggConv1/weights(1x1x128x128,16.38k/16.38kparams)

AggregationLayer/AggConv2(--/147.97kparams)

AggregationLayer/AggConv2/BatchNorm(--/512params)

AggregationLayer/AggConv2/BatchNorm/beta(128,128/128params)

AggregationLayer/AggConv2/BatchNorm/gamma(128,128/128params)

AggregationLayer/AggConv2/BatchNorm/moving_mean(128,128/128params)

AggregationLayer/AggConv2/BatchNorm/moving_variance(128,128/128params)

AggregationLayer/AggConv2/weights(3x3x128x128,147.46k/147.46kparams)

AggregationLayer/AggConv3(--/16.38kparams)

AggregationLayer/AggConv3/weights(1x1x128x128,16.38k/16.38kparams)

```

>同理，_TFProfRoot(--/2.39mparams)即为网络模型整体的参数量，>AggregationLayer/AggConv3/weights(1x1x128x128,16.38k/16.38kparams) 即为网络模型中>AggregationLayer/AggConv3/weights这一层的参数量；

2.2、Pytorch中统计参数量和计算量：

1、针对torchvision自带模型的计算量统计：

对于torchvision中自带的模型，Flops的统计可以按照如下方法，比较轻松的获得：
```
from   torchvision.models import resnet50

from thop import profile

 
model   = resnet50()

input = torch.randn(1, 3, 224, 224)

flops,   params = profile(model, inputs=(input, ))
```
最后输出的计算量和参数量如下：即为2.91GFLOPs和789万的参数量
```
flops: 2914598912.0

parameters: 7978856.0
```
2、使用Fvcore库统计计算量：

目前很多开源的统计flops的工具支持pytorch内置层的flops,不能有效的统计自定义操作的flops；Facebook开源的一个pytorch的cv工具，可以比较轻松支持细粒度的flops统计：

使用 from fvcore.nn.flop_count 函数，可以参考https://github.com/facebookresearch/fvcore/blob/master/tests/test_flop_count.py 中的使用法师，即可简单的统计出网络模型的计算量；

3、使用torchstat统计计算量

torchstat的使用说明参考：https://github.com/Swall0w/torchstat

在定义好网络(ModelNet)后得到model,使用stat ,传入model和输入的shape即可：
```
model = ModelNet(is_train=False)
    stat(model,(512, 512,3))
```
4、统计pytorch的参数量

参数量的统计如下，即可比较简单的统计出参数量；
```
def   params_count(model):
        """
        Compute the number of   parameters.
        Args:
            model (model): model to count   the number of parameters.
        """

    return np.sum([p.numel() for p in   model.parameters()]).item()
```
